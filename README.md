# ğŸ“ NLP Text Classification with Logistic Regression & Naive Bayes

## ğŸ“š Course Information

- **Course Name:** Deep Learning  
- **Lab Title:** NLP Techniques for Text Classification
- 
---

## ğŸ¯ Objective

To implement and compare NLP-based text classification techniques using Logistic Regression and Naive Bayes. The assignment focuses on preprocessing textual data, transforming it into numerical form, and evaluating model performance.

---

## ğŸ“– Brief Theory

### ğŸ”¹ Natural Language Processing (NLP)
NLP enables machines to understand and interpret human language. It involves techniques like:
- **Tokenization** â€“ Breaking text into words or tokens.
- **Stopword Removal** â€“ Filtering out common words like "the", "and", etc.
- **Stemming** â€“ Reducing words to their root form (e.g., â€œplayingâ€ â†’ â€œplayâ€).
- **Lemmatization** â€“ More accurate word reduction using linguistic knowledge.

### ğŸ”¹ Text Vectorization
Machines can't understand raw text, so we convert it into numerical vectors using:
- **CountVectorizer** â€“ Counts word frequency in a document.
- **TF-IDF (Term Frequency-Inverse Document Frequency)** â€“ Considers how important a word is across documents.

### ğŸ”¹ Classification Algorithms
- **Logistic Regression** â€“ A linear model used for binary or multiclass classification.
- **Multinomial Naive Bayes** â€“ A probabilistic model based on Bayes' Theorem, good for text data.

---

## ğŸ—‚ Dataset

- **Dataset Used:** AG News Dataset  
- **Source:** [Kaggle](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)  
- **Classes:** 4 (World, Sports, Business, Sci/Tech)  
- **Attributes:** Title, Description, Label (Class)  

---

## âš™ï¸ Project Structure

```bash
â”œâ”€â”€ ag_news/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ NLP_Text_Classification_SanikaKundekar.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ Output Screenshots/
